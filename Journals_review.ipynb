{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfzFcco1zBfMQF04PhSqSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spkkarri/Tools/blob/main/Journals_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRzJ9RFrOMlK"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests pandas\n",
        "#openalex cors scholarly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "Search_qwuery = \"Quantum Algorithm foundations for Machine learning\"\n",
        "def reconstruct_abstract(inverted_index):\n",
        "    if not isinstance(inverted_index, dict):\n",
        "        return \"\"\n",
        "    # Flatten and collect all (position, word) pairs\n",
        "    words = []\n",
        "    for word, positions in inverted_index.items():\n",
        "        for pos in positions:\n",
        "            words.append((pos, word))\n",
        "    # Sort by position and join\n",
        "    words.sort()\n",
        "    return \" \".join([w for _, w in words])\n",
        "\n",
        "\n",
        "\n",
        "def search_openalex_all_pages(query, per_page=200):\n",
        "    \"\"\"\n",
        "    Search OpenAlex API for all works matching a query in title and abstract.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search string.\n",
        "        per_page (int): Number of results per page (max 200).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns 'title' and 'abstract'.\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.openalex.org/works\"\n",
        "    params = {\n",
        "        \"per_page\": per_page,\n",
        "        \"sort\": \"relevance_score:desc\",\n",
        "        \"filter\": f\"title_and_abstract.search:{query.replace(' ', '+')}\"\n",
        "    }\n",
        "\n",
        "    records = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        params[\"page\"] = page\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Stop if no more results\n",
        "        if not data.get(\"results\"):\n",
        "            break\n",
        "\n",
        "        for result in data.get(\"results\", []):\n",
        "            title = result.get(\"title\", \"\")\n",
        "            abstract = result.get(\"abstract_inverted_index\", \"\")\n",
        "            abstract_text = reconstruct_abstract(abstract)\n",
        "            records.append({\"title\": title, \"abstract\": abstract_text})\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "XiIRDxBXOnps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def search_openalex(query, per_page=10, page=1):\n",
        "    \"\"\"\n",
        "    Search OpenAlex API for works matching a query in title and abstract.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search string.\n",
        "        per_page (int): Number of results per page.\n",
        "        page (int): Page number.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns 'title' and 'abstract'.\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.openalex.org/works\"\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"per_page\": per_page,\n",
        "        \"sort\": \"relevance_score:desc\",\n",
        "        \"filter\": f\"title_and_abstract.search:{query.replace(' ', '+')}\"\n",
        "    }\n",
        "    time.sleep(6)\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract title and abstract\n",
        "    records = []\n",
        "    for result in data.get(\"results\", []):\n",
        "        title = result.get(\"title\", \"\")\n",
        "        abstract = result.get(\"abstract_inverted_index\", \"\")\n",
        "        # OpenAlex returns abstract as an inverted index; reconstruct if present\n",
        "        if isinstance(abstract, dict):\n",
        "            # Reconstruct abstract from inverted index\n",
        "            abstract_words = sorted(abstract.items(), key=lambda x: min(x[1]))\n",
        "            abstract_text = \" \".join(word for word, _ in abstract_words)\n",
        "        else:\n",
        "            abstract_text = abstract or \"\"\n",
        "        records.append({\"title\": title, \"abstract\": abstract_text})\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "s0LK01FySCfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Search for \"agentic retrieval augmented generation\"\n",
        "\n",
        "#https://api.openalex.org/works?page=1&filter=title_and_abstract.search:agentic+retrieval+augmented+generation&sort=relevance_score:desc&per_page=10\n",
        "\n",
        "df = search_openalex_all_pages(Search_qwuery)\n",
        "print(len(df))  # Now this will be the total number of results\n",
        "print(df.head())\n",
        "#save df data frame\n",
        "df.to_pickle(\"openalex_responses.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go_zhq3POpMa",
        "outputId": "10e2c483-8e25-49fe-ec7f-6102bf98fff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "                                               title  \\\n",
            "0  Quantum Machine Learning in Feature Hilbert Sp...   \n",
            "1  Adaptive machine learning framework to acceler...   \n",
            "2             Machine Learning: Quantum vs Classical   \n",
            "3  Enhancing Generative Models via Quantum Correl...   \n",
            "4  Logical Approaches to Computational Barriers: ...   \n",
            "\n",
            "                                            abstract  \n",
            "0  A basic idea of quantum computing is surprisin...  \n",
            "1  Quantum mechanics‚Äêbased ab initio molecular dy...  \n",
            "2  Encouraged by growing computing power and algo...  \n",
            "3  Generative modeling using samples drawn from t...  \n",
            "4  The 12 papers in this special issue arose from...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CORE_API_KEY = \"xxxxxxxxxxxxxxx\"  # Replace with your CORE API key\n",
        "\n",
        "\n",
        "def search_core(query, page=1, page_size=10):\n",
        "    base_url = \"https://api.core.ac.uk/v3/search/works\"\n",
        "    headers = {\"Authorization\": f\"Bearer {CORE_API_KEY}\"}\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"page\": page,\n",
        "        \"pageSize\": page_size,\n",
        "        \"fulltext\": \"true\"  # Only return works with fulltext available\n",
        "    }\n",
        "    response = requests.get(base_url, headers=headers, params=params)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def download_file(url, filename):\n",
        "    time.sleep(6)\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "def core_results_to_df_and_download(results, output_dir):\n",
        "    records = []\n",
        "    for idx, work in enumerate(results.get(\"results\", [])):\n",
        "        title = work.get(\"title\", \"\")\n",
        "        abstract = work.get(\"abstract\", \"\")\n",
        "        download_url = work.get(\"downloadUrl\", \"\")\n",
        "        # Sanitize filename\n",
        "        safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "        filename = f\"{output_dir}/{idx}_{safe_title[:50]}.pdf\"  # Adjust extension if needed\n",
        "        records.append({\n",
        "            \"title\": title,\n",
        "            \"abstract\": abstract,\n",
        "            \"download_url\": download_url,\n",
        "            \"filename\": filename if download_url else \"\"\n",
        "        })\n",
        "        # Download the file if URL is present\n",
        "        if download_url:\n",
        "            try:\n",
        "                download_file(download_url, filename)\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download {download_url}: {e}\")\n",
        "                records[-1][\"filename\"] = \"\"  # Mark as failed\n",
        "    return pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "uloAYAJoPOVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_core_page(query, page=1, page_size=10):\n",
        "    base_url = \"https://api.core.ac.uk/v3/search/works\"\n",
        "    headers = {\"Authorization\": f\"Bearer {CORE_API_KEY}\"}\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"page\": page,\n",
        "        \"pageSize\": page_size,\n",
        "        \"fulltext\": \"true\"\n",
        "    }\n",
        "    time.sleep(6)\n",
        "    response = requests.get(base_url, headers=headers, params=params)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "data = search_core_page(Search_qwuery)\n",
        "print(\"Total hits:\", data.get(\"totalHits\", 0))\n",
        "print(\"Results on this page:\", len(data.get(\"results\", [])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNhBzUpdkQ8",
        "outputId": "bf8c5867-af68-405e-bd4a-5a43b69bace5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total hits: 23870\n",
            "Results on this page: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "OUTPUT_DIR = \"core_downloads\"  # Folder to save downloaded files\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "results = search_core(Search_qwuery)\n",
        "df = core_results_to_df_and_download(results, OUTPUT_DIR)\n",
        "print(len(results))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsDmtCwtY59V",
        "outputId": "d7917454-2860-4a2a-f45d-e312a4ecaf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: core_downloads/0_Quantum machine learning a classical perspective.pdf\n",
            "Downloaded: core_downloads/1_A Quantum Computational Learning Algorithm.pdf\n",
            "Downloaded: core_downloads/2_Natural evolution strategies and variational Monte.pdf\n",
            "Downloaded: core_downloads/3_Micro-Data Learning The Other End of the Spectrum.pdf\n",
            "5\n",
            "                                               title  \\\n",
            "0  Quantum machine learning: a classical perspective   \n",
            "1         A Quantum Computational Learning Algorithm   \n",
            "2  Natural evolution strategies and variational M...   \n",
            "3  Micro-Data Learning: The Other End of the Spec...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  Recently, increased computational power and da...   \n",
            "1  An interesting classical result due to Jackson...   \n",
            "2  A notion of quantum natural evolution strategi...   \n",
            "3  Many fields are now snowed under with an avala...   \n",
            "\n",
            "                                download_url  \\\n",
            "0  https://core.ac.uk/download/154747677.pdf   \n",
            "1      http://arxiv.org/abs/quant-ph/9807052   \n",
            "2            http://arxiv.org/abs/2005.04447   \n",
            "3   https://core.ac.uk/download/49332573.pdf   \n",
            "\n",
            "                                            filename  \n",
            "0  core_downloads/0_Quantum machine learning a cl...  \n",
            "1  core_downloads/1_A Quantum Computational Learn...  \n",
            "2  core_downloads/2_Natural evolution strategies ...  \n",
            "3  core_downloads/3_Micro-Data Learning The Other...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_core_all_pages(query, per_page=10):\n",
        "    base_url = \"https://api.core.ac.uk/v3/search/works\"\n",
        "    headers = {\"Authorization\": f\"Bearer {CORE_API_KEY}\"}\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"page\": 1,\n",
        "        \"pageSize\": per_page,\n",
        "        \"fulltext\": \"true\"\n",
        "    }\n",
        "    records = []\n",
        "    while True:\n",
        "        time.sleep(6)\n",
        "        response = requests.get(base_url, headers=headers, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if not data.get(\"results\"):\n",
        "            break\n",
        "        records.extend(data[\"results\"])\n",
        "        params[\"page\"] += 1\n",
        "    return records\n",
        "\n",
        "def download_file(url, filename):\n",
        "    time.sleep(6)\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "def prepare_and_download(results, output_dir):\n",
        "    all_records = []\n",
        "    downloadable_records = []\n",
        "    for idx, work in enumerate(results):\n",
        "        title = work.get(\"title\", \"\")\n",
        "        abstract = work.get(\"abstract\", \"\")\n",
        "        download_url = work.get(\"downloadUrl\", \"\")\n",
        "        # Sanitize filename\n",
        "        safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "        filename = f\"{output_dir}/{idx}_{safe_title[:50]}.pdf\" if download_url else \"\"\n",
        "        record = {\n",
        "            \"title\": title,\n",
        "            \"abstract\": abstract,\n",
        "            \"download_url\": download_url,\n",
        "            \"filename\": filename\n",
        "        }\n",
        "        all_records.append(record)\n",
        "        if download_url:\n",
        "            downloadable_records.append(record)\n",
        "            try:\n",
        "                download_file(download_url, filename)\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download {download_url}: {e}\")\n",
        "                downloadable_records[-1][\"filename\"] = \"\"\n",
        "    return pd.DataFrame(all_records), pd.DataFrame(downloadable_records)\n",
        "\n",
        "results = search_core_all_pages(Search_qwuery)\n",
        "df_all, df_downloadable = prepare_and_download(results, OUTPUT_DIR)\n",
        "\n",
        "print(\"\\033[1mAll results:\\033[0m\")\n",
        "print(df_all)\n",
        "print(\"\\033[1mDownloadable results:\\033[0m\")\n",
        "print(df_downloadable)\n",
        "print(f\"\\033[1mTotal results:\\033[0m {len(df_all)}\")\n",
        "print(f\"\\033[1mDownloadable results:\\033[0m {len(df_downloadable)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6tPjIpTZfUUY",
        "outputId": "1376b8d1-4f76-4287-9854-79da39800cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://api.core.ac.uk/v3/search/works/?q=Quantum+Algorithm+foundations+for+Machine+learning&page=3&pageSize=10&fulltext=true",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-031ec624e31d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloadable_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_core_all_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSearch_qwuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_downloadable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_and_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-031ec624e31d>\u001b[0m in \u001b[0;36msearch_core_all_pages\u001b[0;34m(query, per_page)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api.core.ac.uk/v3/search/works/?q=Quantum+Algorithm+foundations+for+Machine+learning&page=3&pageSize=10&fulltext=true"
          ]
        }
      ]
    }
  ]
}